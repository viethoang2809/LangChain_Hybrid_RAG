"""
Hybrid RAG BATCH: cháº¡y nhiá»u cÃ¢u há»i giá»‘ng há»‡t CLI vÃ  lÆ°u káº¿t quáº£ ra CSV.
"""
import os, sys, csv, time, asyncio, traceback
from datetime import datetime
from dotenv import load_dotenv
from openai import OpenAI

# === ThÃªm Ä‘Æ°á»ng dáº«n Ä‘á»ƒ import ===
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

# === Import ná»™i bá»™ ===
from app.retrievers.hybrid_retriever import HybridRetrieverParallel
from app.retrievers.vector_tools import VectorClient
from app.utils.hybrid_helpers import (
    load_answer_rule,
    build_id_map_from_graph_records,
    select_topN_by_priority,
    build_synthesis_input,
    llm_summarize_answer,
)

# === Cáº¥u hÃ¬nh ===
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
client = OpenAI(api_key=OPENAI_API_KEY)

INPUT_PATH = "data/Question.csv"
OUTPUT_DIR = "results"
os.makedirs(OUTPUT_DIR, exist_ok=True)
OUTPUT_PATH = os.path.join(
    OUTPUT_DIR,
    f"batch_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
)


# =======================================================
# ğŸš€ CHáº Y 1 CÃ‚U Há»I GIá»NG Há»†T CLI
# =======================================================
async def run_query_once(user_query: str, top_k: int = 10, limit: int = 3, show_debug: bool = False):
    synth_rule = load_answer_rule()
    hybrid = HybridRetrieverParallel()
    vclient = hybrid.vector

    print(f"\nâ“ {user_query}\n")
    print("â³ Äang truy váº¥n dá»¯ liá»‡u song song tá»« Neo4j vÃ  FAISS...\n")

    total_start = time.time()
    hybrid_start = time.time()
    hybrid_result = await hybrid.search(user_query=user_query, top_k=top_k)
    hybrid_time = int((time.time() - hybrid_start) * 1000)

    graph_records = hybrid_result["graph_records"]
    graph_ids = hybrid_result["graph_ids"]
    vector_passages = hybrid_result["vector_passages"]

    # XÃ¢y map Graph
    graph_id_map = build_id_map_from_graph_records(graph_records)

    # Chá»n topN passage theo ID
    fusion_start = time.time()
    chosen_passages = select_topN_by_priority(
        graph_ids, vector_passages, vclient, graph_id_map, fill_limit=limit
    )
    fusion_time = int((time.time() - fusion_start) * 1000)

    # Debug náº¿u cáº§n
    if show_debug:
        print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        print("ğŸ” DEBUG THÃ”NG TIN TRUY Váº¤N")
        print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        print(f"ğŸ“Š Graph IDs ({len(graph_ids)}): {graph_ids[:10]}")
        print(f"ğŸ“š Vector IDs ({len(vector_passages)}): {[p.id for p in vector_passages[:10]]}")
        print(f"âœ… Chosen IDs ({len(chosen_passages)}): {[p.id for p in chosen_passages]}")
        print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n")

    # Chuáº©n bá»‹ dá»¯ liá»‡u tá»•ng há»£p
    synthesis_payload = build_synthesis_input(chosen_passages, graph_id_map)

    # Gá»i LLM tá»•ng há»£p 
    print("ğŸ§  Äang tá»•ng há»£p cÃ¢u tráº£ lá»i báº±ng GPT...\n")
    llm_start = time.time()
    answer = llm_summarize_answer(client, user_query, synth_rule, synthesis_payload, OPENAI_MODEL)
    llm_time = int((time.time() - llm_start) * 1000)
    total_time = int((time.time() - total_start) * 1000)

    # === In ra giá»‘ng CLI ===
    print("\nâœ¨ CÃ‚U TRáº¢ Lá»œI:\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print(answer)
    print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    # In ra cÃ¡c snippet gá»£i nhá»›
    print("\nğŸ“‹ Dá»® LIá»†U Há»¢P NHáº¤T:")
    for p in chosen_passages:
        pid = str(p.id).strip() if p.id else "N/A"
        snippet = (p.text or "").strip().replace("\n", " ")
        if len(snippet) > 120:
            snippet = snippet[:120] + "..."
        print(f"â€¢ ID {pid}: {snippet}...")

    print("\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print("â± THá»œI GIAN Xá»¬ LÃ")
    print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print(f"ğŸ”¸ Graph + Vector: {hybrid_time} ms")
    print(f"ğŸ”¸ Fusion chá»n topN: {fusion_time} ms")
    print(f"ğŸ”¸ LLM tá»•ng há»£p: {llm_time} ms")
    print(f"âš¡ Tá»•ng thá»i gian: {total_time} ms")
    print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n")

    return answer.strip()


# =======================================================
# ğŸ” CHáº Y NHIá»€U CÃ‚U Há»I TRONG FILE
# =======================================================
async def main():
    print("ğŸ  Hybrid RAG â€“ Batch Mode (y há»‡t CLI)")
    print("=========================================================")
    print(f"ğŸ“‚ Äá»c file cÃ¢u há»i: {INPUT_PATH}")

    # Äá»c danh sÃ¡ch cÃ¢u há»i
    with open(INPUT_PATH, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        questions = [row["question"] for row in reader if row.get("question")]

    print(f"âœ… Tá»•ng sá»‘ cÃ¢u há»i: {len(questions)}\n")

    results = []

    for idx, query in enumerate(questions, 1):
        print(f"ğŸ”¹ [{idx}/{len(questions)}] {query}")
        try:
            answer = await run_query_once(query, top_k=10, limit=3)
            results.append({"id": idx, "question": query, "answer": answer})
        except Exception as e:
            print(f"âŒ Lá»—i á»Ÿ cÃ¢u {idx}: {e}")
            print(traceback.format_exc())
            results.append({"id": idx, "question": query, "answer": f"ERROR: {e}"})

    # Ghi ra CSV
    with open(OUTPUT_PATH, "w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=["id", "question", "answer"])
        writer.writeheader()
        writer.writerows(results)

    print("\nâœ… HoÃ n táº¥t! Káº¿t quáº£ lÆ°u táº¡i:")
    print(f"ğŸ‘‰ {os.path.abspath(OUTPUT_PATH)}")


if __name__ == "__main__":
    asyncio.run(main())
